import OpenAI from 'openai'
import { collection, getDocs, query, where, Timestamp } from 'firebase/firestore'
import { db } from '@/app/firebase'
import { NextRequest, NextResponse } from 'next/server'

interface Post {
  content: string
  createdAt: string
  mm: Timestamp
}

const openai = new OpenAI({
  apiKey: process.env.NEXT_OPENAI_API_KEY,
})

function cosineSimilarity(vecA: number[], vecB: number[]) {
  const dot = vecA.reduce((sum, a, i) => sum + a * vecB[i], 0)
  const magA = Math.sqrt(vecA.reduce((sum, a) => sum + a * a, 0))
  const magB = Math.sqrt(vecB.reduce((sum, b) => sum + b * b, 0))
  return dot / (magA * magB)
}

async function getEmbedding(text: string) {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-large',
    input: text,
  })
  return response.data[0].embedding
}

async function fetchCommunityPosts(path: string): Promise<Post[]> {
  const sevenDaysAgo = Date.now() - 7 * 24 * 60 * 60 * 1000
  console.log(sevenDaysAgo)
  const postsQuery = query(
    collection(db, path),
    where('mm', '>=', sevenDaysAgo)
  )
  const postSnapshot = await getDocs(postsQuery)
  return postSnapshot.docs.map(doc => doc.data() as Post)
}

export async function GET(_req: NextRequest) {
  const { searchParams } = new URL(_req.url)
  const path = searchParams.get('path')

  try {
    const posts = await fetchCommunityPosts(path!)

    if (posts.length === 0) {
      return NextResponse.json({ summary: 'ìµœê·¼ 7ì¼ê°„ ì‘ì„±ëœ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.' })
    }

    const defaultQuery = 'ì¸ì²œ ì»¤ë®¤ë‹ˆí‹° íŠ¸ë Œë“œ ë¶„ì„'
    const queryEmbedding = await getEmbedding(defaultQuery)

    const postsWithEmbeddings = await Promise.all(
      posts.map(async post => {
        return {
          ...post,
          embedding: await getEmbedding(post.content),
        }
      })
    )

    const topPosts = postsWithEmbeddings
      .map(post => ({
        ...post,
        score: cosineSimilarity(queryEmbedding, post.embedding),
      }))
      .sort((a, b) => b.score - a.score)
      .slice(0, 5)

    const context = topPosts.map(p => `- ${p.content}`).join('\n')
    const prompt = `ë„ˆëŠ” ì§€ì—­ ì»¤ë®¤ë‹ˆí‹°ì˜ ê¸€ë“¤ì„ ë¶„ì„í•´ì„œ íŠ¸ë Œë“œë¥¼ ìš”ì•½í•˜ëŠ” AIì•¼.
ì•„ë˜ëŠ” ì§€ë‚œ ì¼ì£¼ì¼ ë™ì•ˆ ì¸ì²œ ì»¤ë®¤ë‹ˆí‹°ì— ì˜¬ë¼ì˜¨ ê¸€ ëª©ë¡ì´ì•¼:

${context}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ 4ê°€ì§€ ì¹´í…Œê³ ë¦¬ë¡œ ë‚˜ëˆ ì„œ ìš”ì•½í•´ì¤˜:
1. ğŸ¯ ì¼ìƒ / ì¡ë‹´
2. ğŸ§© ì •ë³´ / íŒ
3. ğŸš§ ë¶ˆí¸ / ê±´ì˜
4. ğŸ  ìƒí™œ / í™˜ê²½

ê° ì¹´í…Œê³ ë¦¬ë§ˆë‹¤ 1~2ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ì„ ì •ë¦¬í•´ì£¼ê³ , ì •ë¦¬ í›„ í•œ ì¤„ ë„ì–´ì¤˜. ì¶”ê°€ì ì¸ ìš”ì•½ì´ë‚˜ ê²°ë¡  ë¬¸ì¥ì€ ì‘ì„±í•˜ì§€ ë§ˆ.
ë˜ ìš”ì•½ë¬¸ì¥ ì•ˆì— ê´„í˜¸ ê°™ì€ ë¬¸ì¥ë¶€í˜¸ëŠ” ì“°ì§€ ë§ì•„ì¤˜. ë¬¸ì¥ë¶€í˜¸ëŠ” ì‰¼í‘œì™€ ë§ˆì¹¨í‘œë§Œ í—ˆìš©ë¼.
ê·¸ë¦¬ê³  ~ì…ë‹ˆë‹¤ ì²´ë¡œ ë¬¸ì¥ì„ ì‘ì„±í•´ì¤˜.`

    const completion = await openai.chat.completions.create({
      model: 'gpt-3.5-turbo',
      messages: [
        { role: 'system', content: 'You are a helpful assistant that summarizes community trends.' },
        { role: 'user', content: prompt },
      ],
    })

    const summary = completion.choices[0].message.content

    return NextResponse.json({ summary })
  } catch (error) {
    console.error('API ì—ëŸ¬:', error)
    return NextResponse.json({ error: 'Internal Server Error' }, { status: 500 })
  }
}
